# ü§ñ Guia de Configura√ß√£o Ollama no Docker

## üìã O Que Foi Configurado

### 1. Servi√ßo Ollama Adicionado ao Docker Compose

Foi adicionado um novo servi√ßo `ollama` no `docker-compose.yml`:

```yaml
ollama:
  image: ollama/ollama:latest
  container_name: sdc-ollama
  runtime: nvidia  # Usa GPU NVIDIA para acelera√ß√£o
  ports:
    - "11434:11434"
  volumes:
    - ollama_data:/root/.ollama  # Persiste os modelos
```

**Caracter√≠sticas**:
- ‚úÖ Usa GPU NVIDIA para infer√™ncia r√°pida
- ‚úÖ Limite de 8GB de RAM
- ‚úÖ Volume persistente para modelos (n√£o precisa baixar sempre)
- ‚úÖ Health check autom√°tico
- ‚úÖ Conectado √† mesma rede do backend

### 2. Backend Configurado para Usar Ollama no Docker

Arquivo `backend/python/gerador_ata/config_ata.py` atualizado:

```python
OLLAMA_BASE_URL = os.getenv('OLLAMA_BASE_URL', 'http://ollama:11434')
OLLAMA_MODEL = os.getenv('OLLAMA_MODEL', 'gemma2:9b')
```

Agora usa o nome do servi√ßo Docker `ollama` ao inv√©s de `localhost`.

### 3. Modelo Configurado

**Modelo principal**: `gemma2:9b` (5.4 GB)

**Outros modelos dispon√≠veis** (opcionais):
- qwen3:8b (5.2 GB)
- llama3.1:8b (4.9 GB)
- gpt-oss:20b (13 GB)

## üöÄ Passo a Passo para Ativar

### Op√ß√£o 1: Script Automatizado (Recomendado)

```bash
cd /home/jonathanbarbosa/dev/SDC-Transcreve

# Executar script de migra√ß√£o
./migrate-ollama-models.sh
```

O script vai:
1. ‚úÖ Verificar se container Ollama est√° rodando
2. ‚úÖ Perguntar quais modelos copiar (s√≥ gemma2:9b ou todos)
3. ‚úÖ Fazer download dos modelos para o Docker
4. ‚úÖ Verificar se tudo funcionou

### Op√ß√£o 2: Manual

```bash
# 1. Subir o servi√ßo Ollama
docker compose up -d ollama

# 2. Aguardar container iniciar (30-40s)
sleep 30

# 3. Fazer pull do modelo principal
docker exec sdc-ollama ollama pull gemma2:9b

# 4. Verificar se modelo foi instalado
docker exec sdc-ollama ollama list

# 5. Testar modelo
docker exec sdc-ollama ollama run gemma2:9b "Ol√°, como voc√™ est√°?"
```

### Op√ß√£o 3: Copiar Modelos J√° Baixados do Windows (Avan√ßado)

Se voc√™ n√£o quer baixar novamente (economizar tempo/banda):

```powershell
# No PowerShell (Windows):

# 1. Encontrar onde est√£o os modelos no Windows
$ollamaPath = "$env:USERPROFILE\.ollama\models"
echo $ollamaPath

# 2. Criar arquivo tar com os modelos
cd $ollamaPath
tar -czf ollama-models.tar.gz blobs manifests

# 3. Copiar para WSL
wsl cp "/mnt/c/Users/<SEU_USUARIO>/.ollama/models/ollama-models.tar.gz" /tmp/

# No WSL (Bash):
# 4. Copiar para o volume Docker
docker compose up -d ollama
sleep 10

# 5. Extrair modelos no container
docker cp /tmp/ollama-models.tar.gz sdc-ollama:/root/.ollama/
docker exec sdc-ollama tar -xzf /root/.ollama/ollama-models.tar.gz -C /root/.ollama/
docker exec sdc-ollama rm /root/.ollama/ollama-models.tar.gz

# 6. Verificar
docker exec sdc-ollama ollama list
```

## üîÑ Rebuild da Aplica√ß√£o

Ap√≥s configurar o Ollama, fa√ßa rebuild:

```bash
cd /home/jonathanbarbosa/dev/SDC-Transcreve
./docker-commands.sh rebuild
```

## üß™ Testar Gera√ß√£o de Ata

### 1. Verificar Status do Ollama

**Via API do Backend**:
```bash
curl http://localhost:8000/api/gerar-ata/ollama/status
```

**Resposta esperada**:
```json
{
  "status": "online",
  "modelo_configurado": "gemma2:9b",
  "modelos_disponiveis": ["gemma2:9b"],
  "url": "http://ollama:11434"
}
```

### 2. Testar Diretamente no Container

```bash
# Testar modelo com prompt simples
docker exec sdc-ollama ollama run gemma2:9b "Resuma: A reuni√£o foi sobre planejamento de projeto."
```

### 3. Testar via Frontend

1. Acesse o frontend: `http://localhost/`
2. V√° para "Gerar Ata"
3. Preencha os dados da reuni√£o
4. Clique em "Gerar Ata"
5. Aguarde o processamento (pode levar 1-3 minutos dependendo do tamanho)

## üìä Monitoramento

### Ver Logs do Ollama

```bash
# Logs em tempo real
docker logs -f sdc-ollama

# √öltimas 100 linhas
docker logs --tail 100 sdc-ollama
```

### Verificar Uso de GPU

```bash
# Ver GPU sendo usada pelo Ollama
nvidia-smi

# Monitoramento cont√≠nuo
watch -n 2 nvidia-smi
```

### Ver Modelos Instalados

```bash
docker exec sdc-ollama ollama list
```

**Sa√≠da esperada**:
```
NAME            ID              SIZE      MODIFIED
gemma2:9b       ff02c3702f32    5.4 GB    5 minutes ago
```

## ‚öôÔ∏è Configura√ß√µes Avan√ßadas

### Vari√°veis de Ambiente (docker-compose.yml)

Voc√™ pode adicionar no servi√ßo `backend`:

```yaml
backend:
  environment:
    # Ollama
    OLLAMA_BASE_URL: http://ollama:11434
    OLLAMA_MODEL: gemma2:9b
    OLLAMA_TIMEOUT: 600
    OLLAMA_TEMPERATURE: 0.1
    OLLAMA_MAX_TOKENS: 6000
    OLLAMA_TOP_P: 0.8
```

### Trocar de Modelo

Para usar outro modelo (ex: llama3.1:8b):

```bash
# 1. Baixar modelo
docker exec sdc-ollama ollama pull llama3.1:8b

# 2. Editar config_ata.py ou adicionar vari√°vel de ambiente
# No docker-compose.yml, se√ß√£o backend:
environment:
  OLLAMA_MODEL: llama3.1:8b

# 3. Rebuild
./docker-commands.sh rebuild
```

### Ajustar Par√¢metros de Gera√ß√£o

Edite `backend/python/gerador_ata/config_ata.py`:

```python
OLLAMA_TEMPERATURE = 0.1    # Menor = mais focado, maior = mais criativo (0.0-1.0)
OLLAMA_MAX_TOKENS = 6000    # M√°ximo de tokens a gerar
OLLAMA_TOP_P = 0.8          # Nucleus sampling (0.0-1.0)
OLLAMA_TIMEOUT = 600        # Timeout em segundos (10 minutos)
```

## üîß Solu√ß√£o de Problemas

### Problema 1: Container Ollama n√£o inicia

**Sintomas**:
```
Error response from daemon: could not select device driver "" with capabilities: [[gpu]]
```

**Solu√ß√£o**:
```bash
# Verificar se nvidia-docker est√° instalado
dpkg -l | grep nvidia-docker2

# Se n√£o estiver, instalar:
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

### Problema 2: Ollama est√° offline

**Verificar**:
```bash
# Status do container
docker ps | grep ollama

# Logs
docker logs sdc-ollama

# Testar health
docker exec sdc-ollama curl http://localhost:11434/api/tags
```

**Solu√ß√£o**:
```bash
# Reiniciar container
docker compose restart ollama

# Ou recriar
docker compose up -d --force-recreate ollama
```

### Problema 3: Modelo n√£o encontrado

**Erro**:
```
Error: model 'gemma2:9b' not found
```

**Solu√ß√£o**:
```bash
# Listar modelos
docker exec sdc-ollama ollama list

# Se estiver vazio, fazer pull
docker exec sdc-ollama ollama pull gemma2:9b
```

### Problema 4: Gera√ß√£o muito lenta

**Poss√≠veis causas**:
1. **N√£o est√° usando GPU**: Verificar `nvidia-smi`
2. **Modelo muito grande**: Usar modelo menor (llama3.1:8b ao inv√©s de gpt-oss:20b)
3. **Pouca VRAM**: Verificar uso com `nvidia-smi`

**Solu√ß√£o**:
```bash
# Ver se Ollama est√° usando GPU
nvidia-smi

# Trocar para modelo menor se necess√°rio
docker exec sdc-ollama ollama pull llama3.1:8b
# Atualizar OLLAMA_MODEL no config_ata.py
```

### Problema 5: Backend n√£o consegue conectar ao Ollama

**Erro nos logs do backend**:
```
N√£o foi poss√≠vel conectar ao Ollama em http://ollama:11434
```

**Verificar**:
```bash
# Backend e Ollama est√£o na mesma rede?
docker network inspect sdc-network

# Testar conectividade do backend para o ollama
docker exec sdc-transcription-backend curl http://ollama:11434/api/tags
```

**Solu√ß√£o**:
```bash
# Rebuild com depend√™ncias corretas
docker compose down
docker compose up -d
```

## üìà Compara√ß√£o: Windows vs Docker

| Aspecto | Ollama no Windows | Ollama no Docker |
|---------|-------------------|------------------|
| **Desempenho** | Similar | Similar (usa mesma GPU) |
| **Isolamento** | ‚ùå Compartilha recursos | ‚úÖ Isolado, com limites |
| **Portabilidade** | ‚ùå S√≥ funciona no Windows | ‚úÖ Funciona em qualquer host |
| **Gerenciamento** | ‚ùå Manual | ‚úÖ Autom√°tico via compose |
| **Deploy** | ‚ùå Precisa instalar manualmente | ‚úÖ Deploy autom√°tico |
| **RAM** | ‚ùå Sem limite | ‚úÖ Limite de 8GB |
| **Logs** | ‚ùå Dif√≠cil rastrear | ‚úÖ F√°cil com `docker logs` |

## üéØ Checklist de Configura√ß√£o

- [ ] Servi√ßo Ollama adicionado ao `docker-compose.yml`
- [ ] Volume `ollama_data` criado
- [ ] Backend depende do Ollama (`depends_on`)
- [ ] `config_ata.py` atualizado com `OLLAMA_BASE_URL`
- [ ] Container Ollama rodando: `docker ps | grep ollama`
- [ ] Modelo gemma2:9b instalado: `docker exec sdc-ollama ollama list`
- [ ] Health check OK: `docker exec sdc-ollama curl http://localhost:11434/api/tags`
- [ ] Backend conecta ao Ollama: `curl localhost:8000/api/gerar-ata/ollama/status`
- [ ] Rebuild completo: `./docker-commands.sh rebuild`
- [ ] Teste de gera√ß√£o de ata funcionando

## üöÄ Comandos R√°pidos

```bash
# Status
docker compose ps

# Ver modelos
docker exec sdc-ollama ollama list

# Logs
docker logs -f sdc-ollama

# Testar
docker exec sdc-ollama ollama run gemma2:9b "Teste"

# Reinstalar modelo
docker exec sdc-ollama ollama pull gemma2:9b

# Rebuild tudo
./docker-commands.sh rebuild

# Ver uso de GPU
nvidia-smi
```

## üéâ Resultado Final

Com a configura√ß√£o completa:

‚úÖ Ollama roda dentro do Docker com GPU
‚úÖ Backend conecta automaticamente ao Ollama
‚úÖ Modelos persistem (n√£o precisa baixar sempre)
‚úÖ Gera√ß√£o de ata funciona via frontend
‚úÖ Monitoramento f√°cil com Docker
‚úÖ Deploy simplificado (s√≥ um `docker compose up`)

**Uso de mem√≥ria esperado**:
- Ollama: 2-6 GB RAM + 4-8 GB VRAM
- Backend: 2-4 GB RAM + 2-4 GB VRAM (Whisper)
- Total GPU: ~6-12 GB VRAM (depende dos modelos)

**Certifique-se que sua GPU tem pelo menos 12GB VRAM para rodar ambos confortavelmente!**
