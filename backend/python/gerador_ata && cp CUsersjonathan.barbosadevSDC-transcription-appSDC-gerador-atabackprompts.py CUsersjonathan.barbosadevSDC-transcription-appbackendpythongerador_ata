"""
ServiÃ§o de comunicaÃ§Ã£o com Ollama para geraÃ§Ã£o de atas
SDC-Ata-Generator
"""

import requests
import logging
import time
from typing import Tuple

import config
from prompts import criar_prompt_geracao_ata, criar_prompt_correcao_toon
from toon_parser import parse_toon, TOONParserError


logger = logging.getLogger(__name__)


class OllamaServiceError(Exception):
    """ExceÃ§Ã£o para erros do serviÃ§o Ollama"""
    pass


class OllamaService:
    """ServiÃ§o para gerar atas usando Ollama"""

    def __init__(self):
        self.base_url = config.OLLAMA_BASE_URL
        self.model = config.OLLAMA_MODEL
        self.timeout = config.OLLAMA_TIMEOUT
        self.temperature = config.OLLAMA_TEMPERATURE
        self.max_tokens = config.OLLAMA_MAX_TOKENS
        self.top_p = config.OLLAMA_TOP_P

    def check_health(self) -> bool:
        """Verifica se Ollama estÃ¡ online"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except Exception as e:
            logger.warning(f"Ollama nÃ£o disponÃ­vel: {e}")
            return False

    def list_models(self) -> list:
        """Lista modelos disponÃ­veis no Ollama"""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=10)
            if response.status_code == 200:
                data = response.json()
                return [model['name'] for model in data.get('models', [])]
            return []
        except Exception as e:
            logger.error(f"Erro ao listar modelos: {e}")
            return []

    def generate_completion(self, prompt: str) -> str:
        """Envia prompt para Ollama e retorna resposta"""
        try:
            # Log detalhado do prompt
            prompt_size = len(prompt)
            prompt_preview = prompt[:200].replace('\n', ' ')
            logger.info("="*60)
            logger.info(f"ğŸ“Š INICIANDO GERAÃ‡ÃƒO COM OLLAMA")
            logger.info(f"   Modelo: {self.model}")
            logger.info(f"   Tamanho do prompt: {prompt_size:,} caracteres")
            logger.info(f"   Preview: {prompt_preview}...")
            logger.info(f"   Temperature: {self.temperature}")
            logger.info(f"   Top P: {self.top_p}")
            logger.info(f"   Max tokens: {self.max_tokens}")
            logger.info("="*60)

            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": self.temperature,
                    "top_p": self.top_p,
                    "num_predict": self.max_tokens
                }
            }

            # Marcar inÃ­cio da geraÃ§Ã£o
            start_time = time.time()
            logger.info("â±ï¸  Enviando requisiÃ§Ã£o para Ollama...")

            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=self.timeout
            )

            # Calcular tempo decorrido
            elapsed_time = time.time() - start_time

            if response.status_code != 200:
                logger.error(f"âŒ Ollama retornou erro {response.status_code}")
                raise OllamaServiceError(
                    f"Ollama retornou status {response.status_code}: {response.text}"
                )

            data = response.json()
            generated_text = data.get('response', '')

            if not generated_text:
                logger.error("âŒ Ollama retornou resposta vazia")
                raise OllamaServiceError("Ollama retornou resposta vazia")

            # Log detalhado da resposta
            response_size = len(generated_text)
            response_preview = generated_text[:200].replace('\n', ' ')

            # Extrair estatÃ­sticas se disponÃ­veis
            total_duration = data.get('total_duration', 0) / 1_000_000_000  # Converter de ns para s
            eval_count = data.get('eval_count', 0)  # Tokens gerados
            eval_duration = data.get('eval_duration', 0) / 1_000_000_000

            tokens_per_sec = eval_count / eval_duration if eval_duration > 0 else 0

            logger.info("="*60)
            logger.info(f"âœ… RESPOSTA RECEBIDA DO OLLAMA")
            logger.info(f"   Tempo total: {elapsed_time:.2f}s")
            logger.info(f"   Tamanho da resposta: {response_size:,} caracteres")
            logger.info(f"   Tokens gerados: {eval_count}")
            logger.info(f"   Velocidade: {tokens_per_sec:.1f} tokens/s")
            logger.info(f"   Preview: {response_preview}...")
            logger.info("="*60)

            # PRINT COMPLETO DO OUTPUT DA IA
            print("\n" + "ğŸ¤–"*40)
            print("ğŸ¤– OUTPUT COMPLETO DA IA:")
            print("ğŸ¤–"*40)
            print(generated_text)
            print("ğŸ¤–"*40 + "\n")

            return generated_text.strip()

        except requests.exceptions.Timeout:
            raise OllamaServiceError(
                f"Timeout ao aguardar resposta do Ollama ({self.timeout}s)"
            )
        except requests.exceptions.ConnectionError:
            raise OllamaServiceError(
                f"NÃ£o foi possÃ­vel conectar ao Ollama em {self.base_url}"
            )
        except Exception as e:
            raise OllamaServiceError(f"Erro ao gerar completion: {str(e)}")

    def gerar_toon_from_dados(
        self,
        participantes: str,
        data_hora: str,
        local: str,
        convocado_por: str,
        transcricao: str,
        max_retries: int = 2
    ) -> Tuple[str, dict]:
        """
        Gera TOON estruturado a partir dos dados da reuniÃ£o
        """
        # Log inicial com estatÃ­sticas
        transcricao_size = len(transcricao)
        logger.info("\n" + "="*70)
        logger.info("ğŸ¤– INICIANDO GERAÃ‡ÃƒO DE ATA COM IA")
        logger.info("="*70)
        logger.info(f"ğŸ“ Tamanho da transcriÃ§Ã£o: {transcricao_size:,} caracteres")
        logger.info(f"ğŸ‘¥ Participantes: {participantes}")
        logger.info(f"ğŸ“… Data/Hora: {data_hora}")
        logger.info(f"ğŸ“ Local: {local}")
        logger.info(f"ğŸ“¢ Convocado por: {convocado_por}")
        logger.info(f"ğŸ”„ MÃ¡ximo de tentativas: {max_retries + 1}")
        logger.info("="*70 + "\n")

        prompt = criar_prompt_geracao_ata(
            participantes, data_hora, local, convocado_por, transcricao
        )

        logger.info(f"ğŸ“ Prompt criado com {len(prompt):,} caracteres")

        for tentativa in range(max_retries + 1):
            try:
                logger.info(f"\nğŸ”„ TENTATIVA {tentativa + 1}/{max_retries + 1}")
                logger.info("-"*70)

                # Gerar completion
                toon_gerado = self.generate_completion(prompt)

                # Limpar markdown
                logger.info("ğŸ§¹ Limpando markdown da resposta...")
                toon_gerado = self._limpar_markdown(toon_gerado)
                logger.info(f"   TOON limpo: {len(toon_gerado):,} caracteres")

                # PRINT DO TOON LIMPO
                print("\n" + "ğŸ“„"*40)
                print("ğŸ“„ TOON APÃ“S LIMPEZA DE MARKDOWN:")
                print("ğŸ“„"*40)
                print(toon_gerado)
                print("ğŸ“„"*40 + "\n")

                # Log do TOON gerado (primeiras linhas)
                toon_preview = '\n'.join(toon_gerado.split('\n')[:10])
                logger.debug(f"ğŸ“„ Preview do TOON gerado:\n{toon_preview}\n...")

                # Tentar parsear
                logger.info("ğŸ” Parseando TOON...")
                dados_parseados = parse_toon(toon_gerado)

                # Extrair estatÃ­sticas
                num_pontos = len(dados_parseados.get('pontos', []))
                num_proximos_passos = len(dados_parseados.get('proximos_passos', []))

                logger.info("="*70)
                logger.info("âœ… SUCESSO! ATA GERADA COM SUCESSO")
                logger.info(f"   ğŸ“Š Pontos discutidos: {num_pontos}")
                logger.info(f"   ğŸ“‹ PrÃ³ximos passos: {num_proximos_passos}")
                logger.info(f"   ğŸ¯ Tentativa bem-sucedida: {tentativa + 1}/{max_retries + 1}")
                logger.info("="*70 + "\n")

                return toon_gerado, dados_parseados

            except TOONParserError as e:
                logger.warning("="*70)
                logger.warning(f"âš ï¸  ERRO NO PARSE (tentativa {tentativa + 1})")
                logger.warning(f"   Erro: {str(e)}")
                logger.warning("="*70)

                if tentativa < max_retries:
                    logger.info(f"ğŸ”§ Tentando corrigir TOON (tentativa {tentativa + 2}/{max_retries + 1})...")
                    prompt = criar_prompt_correcao_toon(toon_gerado, str(e))
                else:
                    logger.error("="*70)
                    logger.error("âŒ FALHA TOTAL - Todas as tentativas esgotadas")
                    logger.error(f"   Total de tentativas: {max_retries + 1}")
                    logger.error(f"   Ãšltimo erro: {e}")
                    logger.error("="*70)
                    raise OllamaServiceError(
                        f"NÃ£o foi possÃ­vel gerar TOON vÃ¡lido apÃ³s {max_retries + 1} tentativas. "
                        f"Ãšltimo erro: {e}"
                    )

            except OllamaServiceError:
                raise

            except Exception as e:
                logger.error("="*70)
                logger.error(f"âŒ ERRO INESPERADO: {type(e).__name__}")
                logger.error(f"   Detalhes: {str(e)}")
                logger.error("="*70)
                raise OllamaServiceError(f"Erro ao processar: {str(e)}")

        raise OllamaServiceError("Falha ao gerar TOON vÃ¡lido")

    def _limpar_markdown(self, texto: str) -> str:
        """Remove marcadores markdown e texto extra da resposta do LLM"""
        # Remover blocos de cÃ³digo ```
        if '```' in texto:
            linhas = texto.split('\n')
            resultado = []
            dentro_bloco = False

            for linha in linhas:
                stripped = linha.strip()
                if stripped.startswith('```'):
                    dentro_bloco = not dentro_bloco
                    continue
                resultado.append(linha)

            texto = '\n'.join(resultado)

        # Remover linhas que comeÃ§am com # (headers markdown)
        linhas = texto.split('\n')
        linhas = [l for l in linhas if not l.strip().startswith('#')]
        texto = '\n'.join(linhas)

        # Remover ** (bold markdown)
        texto = texto.replace('**', '')

        # Encontrar onde comeÃ§a o TOON real (primeira linha com "local:")
        linhas = texto.split('\n')
        inicio_toon = 0
        for i, linha in enumerate(linhas):
            if linha.strip().startswith('local:'):
                inicio_toon = i
                break

        texto = '\n'.join(linhas[inicio_toon:])

        return texto.strip()


ollama_service = OllamaService()


if __name__ == '__main__':
    logging.basicConfig(level=logging.DEBUG)

    print("Testando OllamaService...")
    service = OllamaService()

    if service.check_health():
        print("âœ“ Ollama estÃ¡ online")
        print(f"âœ“ Modelos: {service.list_models()}")
    else:
        print("âœ— Ollama nÃ£o estÃ¡ disponÃ­vel")
